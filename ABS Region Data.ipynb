{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project ABS\n",
    "The goal of this notebook will be to explore data compiled from the Australian Bureau of Statistics (ABS) compiled as part of their [AUSSTATS](https://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1410.02013-18?OpenDocument) documentation, looking at trends across a wide variety of \"key measures\" by geographical region within Australia.\n",
    "\n",
    "Preliminarily I am interested in exploring ties between income and age (for potential use in later analysis on tax effects on different generations), but there are also some interesting factoids regarding solar panel installations which have piqued my curiousity.\n",
    "\n",
    "Note all files were originally \".xls\" format, and have had incredibly minor alterations as follows:\n",
    "* Removing \"branding\" and \"page header\" rows (rows 1-5 in source document)\n",
    "* Removing \"Copyright\" and empty \"footer\" columns at the end of the dataset\n",
    "* Consolidating all the details outlined in the column headers into 1 header per column (removing merged formatting and cross filling for empty cells)\n",
    "* Saving each document as .csv\n",
    "\n",
    "All files are based on the ASGS (Australian Statistical Geography Standard) location methodology. There is potential of further exploration based on documents located [here](https://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.003July%202011?OpenDocument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cartechr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py:855: MatplotlibDeprecationWarning: \n",
      "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n",
      "  \"found relative to the 'datapath' directory.\".format(key))\n",
      "C:\\Users\\cartechr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py:846: MatplotlibDeprecationWarning: \n",
      "The text.latex.unicode rcparam was deprecated in Matplotlib 2.2 and will be removed in 3.1.\n",
      "  \"2.2\", name=key, obj_type=\"rcparam\", addendum=addendum)\n"
     ]
    }
   ],
   "source": [
    "# Declare Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import seaborn as sns\n",
    "import os\n",
    "from textwrap import wrap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import eli5\n",
    "from cleanfunc import load_merge_clean\n",
    "\n",
    "\n",
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cartechr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2903: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "C:\\Users\\cartechr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "C:\\Users\\cartechr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5430: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\cartechr\\\\OneDrive - Mars Inc\\\\GitHub\\\\ABS-Region-Data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a variable for current notebook's path for various loading/saving mechanisms\n",
    "df = load_merge_clean()\n",
    "\n",
    "nb_path = os.getcwd()\n",
    "nb_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ESTIMATES OF PERSONAL INCOME Employee income earners no.</th>\n",
       "      <th>ESTIMATES OF PERSONAL INCOME Employee income earners - median age  years</th>\n",
       "      <th>ESTIMATES OF PERSONAL INCOME Total Employee income $</th>\n",
       "      <th>ESTIMATES OF PERSONAL INCOME Median Employee income $</th>\n",
       "      <th>ESTIMATES OF PERSONAL INCOME Mean Employee income $</th>\n",
       "      <th>ESTIMATES OF PERSONAL INCOME Employee income as main source of income %</th>\n",
       "      <th>ESTIMATES OF PERSONAL INCOME Own unincorporated business income earners no.</th>\n",
       "      <th>ESTIMATES OF PERSONAL INCOME Own unincorporated business income earners - median age  years</th>\n",
       "      <th>ESTIMATES OF PERSONAL INCOME Total Own unincorporated business income $</th>\n",
       "      <th>ESTIMATES OF PERSONAL INCOME Median Own unincorporated business income $</th>\n",
       "      <th>...</th>\n",
       "      <th>SOCIO-ECONOMIC INDEXES FOR AREAS (SEIFA) Australia SEIFA decile ranking - Index of Economic Resources (IER) no.</th>\n",
       "      <th>SOCIO-ECONOMIC INDEXES FOR AREAS (SEIFA) State SEIFA decile ranking - Index of Relative Socio-Economic Disadvantage (IRSD) no.</th>\n",
       "      <th>SOCIO-ECONOMIC INDEXES FOR AREAS (SEIFA) State SEIFA decile ranking - Index of Relative Socio-Economic Advantage and Disadvantage (IRSAD) no.</th>\n",
       "      <th>SOCIO-ECONOMIC INDEXES FOR AREAS (SEIFA) State SEIFA decile ranking - Index of Education and Occupation (IEO) no.</th>\n",
       "      <th>SOCIO-ECONOMIC INDEXES FOR AREAS (SEIFA) State SEIFA decile ranking - Index of Economic Resources (IER) no.</th>\n",
       "      <th>HOMELESSNESS Homelessness rate per 10,000 persons rate</th>\n",
       "      <th>COMMUTING TO WORK Average commuting distance from Place of Usual Residence kms</th>\n",
       "      <th>COMMUTING TO WORK Median commuting distance from Place of Usual Residence kms</th>\n",
       "      <th>COMMUTING TO WORK Average commuting distance to place of work kms</th>\n",
       "      <th>COMMUTING TO WORK Median commuting distance to place of work kms</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CODE</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>YEAR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">101021007</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">Braidwood</th>\n",
       "      <th>2014</th>\n",
       "      <td>1595.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>77935375.0</td>\n",
       "      <td>41233.0</td>\n",
       "      <td>48862.0</td>\n",
       "      <td>64.2</td>\n",
       "      <td>753.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>4834345.0</td>\n",
       "      <td>1879.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.455036</td>\n",
       "      <td>5.483813</td>\n",
       "      <td>5.485612</td>\n",
       "      <td>5.497307</td>\n",
       "      <td>5.483813</td>\n",
       "      <td>44.959294</td>\n",
       "      <td>18.20107</td>\n",
       "      <td>12.379964</td>\n",
       "      <td>15.532562</td>\n",
       "      <td>8.646964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>1641.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>83909804.0</td>\n",
       "      <td>42960.0</td>\n",
       "      <td>51133.0</td>\n",
       "      <td>62.9</td>\n",
       "      <td>747.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>8810938.0</td>\n",
       "      <td>3861.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.455036</td>\n",
       "      <td>5.483813</td>\n",
       "      <td>5.485612</td>\n",
       "      <td>5.497307</td>\n",
       "      <td>5.483813</td>\n",
       "      <td>44.959294</td>\n",
       "      <td>18.20107</td>\n",
       "      <td>12.379964</td>\n",
       "      <td>15.532562</td>\n",
       "      <td>8.646964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>1652.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>90828866.0</td>\n",
       "      <td>45312.0</td>\n",
       "      <td>54981.0</td>\n",
       "      <td>63.3</td>\n",
       "      <td>728.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>9970076.0</td>\n",
       "      <td>5001.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>52.900000</td>\n",
       "      <td>38.00000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>4.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">101021008</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Karabar</th>\n",
       "      <th>2014</th>\n",
       "      <td>4690.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>268993415.0</td>\n",
       "      <td>54157.0</td>\n",
       "      <td>57355.0</td>\n",
       "      <td>85.2</td>\n",
       "      <td>463.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>9090597.0</td>\n",
       "      <td>11363.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.455036</td>\n",
       "      <td>5.483813</td>\n",
       "      <td>5.485612</td>\n",
       "      <td>5.497307</td>\n",
       "      <td>5.483813</td>\n",
       "      <td>44.959294</td>\n",
       "      <td>18.20107</td>\n",
       "      <td>12.379964</td>\n",
       "      <td>15.532562</td>\n",
       "      <td>8.646964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>4806.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>283280414.0</td>\n",
       "      <td>55369.0</td>\n",
       "      <td>58943.0</td>\n",
       "      <td>84.5</td>\n",
       "      <td>469.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>8387683.0</td>\n",
       "      <td>10503.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.455036</td>\n",
       "      <td>5.483813</td>\n",
       "      <td>5.485612</td>\n",
       "      <td>5.497307</td>\n",
       "      <td>5.483813</td>\n",
       "      <td>44.959294</td>\n",
       "      <td>18.20107</td>\n",
       "      <td>12.379964</td>\n",
       "      <td>15.532562</td>\n",
       "      <td>8.646964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 218 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          ESTIMATES OF PERSONAL INCOME Employee income earners no.  \\\n",
       "CODE      LABEL     YEAR                                                             \n",
       "101021007 Braidwood 2014                                             1595.0          \n",
       "                    2015                                             1641.0          \n",
       "                    2016                                             1652.0          \n",
       "101021008 Karabar   2014                                             4690.0          \n",
       "                    2015                                             4806.0          \n",
       "\n",
       "                          ESTIMATES OF PERSONAL INCOME Employee income earners - median age  years  \\\n",
       "CODE      LABEL     YEAR                                                                             \n",
       "101021007 Braidwood 2014                                               44.0                          \n",
       "                    2015                                               44.0                          \n",
       "                    2016                                               44.0                          \n",
       "101021008 Karabar   2014                                               40.0                          \n",
       "                    2015                                               40.0                          \n",
       "\n",
       "                          ESTIMATES OF PERSONAL INCOME Total Employee income $  \\\n",
       "CODE      LABEL     YEAR                                                         \n",
       "101021007 Braidwood 2014                                         77935375.0      \n",
       "                    2015                                         83909804.0      \n",
       "                    2016                                         90828866.0      \n",
       "101021008 Karabar   2014                                        268993415.0      \n",
       "                    2015                                        283280414.0      \n",
       "\n",
       "                          ESTIMATES OF PERSONAL INCOME Median Employee income $  \\\n",
       "CODE      LABEL     YEAR                                                          \n",
       "101021007 Braidwood 2014                                            41233.0       \n",
       "                    2015                                            42960.0       \n",
       "                    2016                                            45312.0       \n",
       "101021008 Karabar   2014                                            54157.0       \n",
       "                    2015                                            55369.0       \n",
       "\n",
       "                          ESTIMATES OF PERSONAL INCOME Mean Employee income $  \\\n",
       "CODE      LABEL     YEAR                                                        \n",
       "101021007 Braidwood 2014                                            48862.0     \n",
       "                    2015                                            51133.0     \n",
       "                    2016                                            54981.0     \n",
       "101021008 Karabar   2014                                            57355.0     \n",
       "                    2015                                            58943.0     \n",
       "\n",
       "                          ESTIMATES OF PERSONAL INCOME Employee income as main source of income %  \\\n",
       "CODE      LABEL     YEAR                                                                            \n",
       "101021007 Braidwood 2014                                               64.2                         \n",
       "                    2015                                               62.9                         \n",
       "                    2016                                               63.3                         \n",
       "101021008 Karabar   2014                                               85.2                         \n",
       "                    2015                                               84.5                         \n",
       "\n",
       "                          ESTIMATES OF PERSONAL INCOME Own unincorporated business income earners no.  \\\n",
       "CODE      LABEL     YEAR                                                                                \n",
       "101021007 Braidwood 2014                                              753.0                             \n",
       "                    2015                                              747.0                             \n",
       "                    2016                                              728.0                             \n",
       "101021008 Karabar   2014                                              463.0                             \n",
       "                    2015                                              469.0                             \n",
       "\n",
       "                          ESTIMATES OF PERSONAL INCOME Own unincorporated business income earners - median age  years  \\\n",
       "CODE      LABEL     YEAR                                                                                                \n",
       "101021007 Braidwood 2014                                               55.0                                             \n",
       "                    2015                                               55.0                                             \n",
       "                    2016                                               56.0                                             \n",
       "101021008 Karabar   2014                                               47.0                                             \n",
       "                    2015                                               46.0                                             \n",
       "\n",
       "                          ESTIMATES OF PERSONAL INCOME Total Own unincorporated business income $  \\\n",
       "CODE      LABEL     YEAR                                                                            \n",
       "101021007 Braidwood 2014                                          4834345.0                         \n",
       "                    2015                                          8810938.0                         \n",
       "                    2016                                          9970076.0                         \n",
       "101021008 Karabar   2014                                          9090597.0                         \n",
       "                    2015                                          8387683.0                         \n",
       "\n",
       "                          ESTIMATES OF PERSONAL INCOME Median Own unincorporated business income $  \\\n",
       "CODE      LABEL     YEAR                                                                             \n",
       "101021007 Braidwood 2014                                             1879.0                          \n",
       "                    2015                                             3861.0                          \n",
       "                    2016                                             5001.0                          \n",
       "101021008 Karabar   2014                                            11363.0                          \n",
       "                    2015                                            10503.0                          \n",
       "\n",
       "                                                        ...                                 \\\n",
       "CODE      LABEL     YEAR                                ...                                  \n",
       "101021007 Braidwood 2014                                ...                                  \n",
       "                    2015                                ...                                  \n",
       "                    2016                                ...                                  \n",
       "101021008 Karabar   2014                                ...                                  \n",
       "                    2015                                ...                                  \n",
       "\n",
       "                          SOCIO-ECONOMIC INDEXES FOR AREAS (SEIFA) Australia SEIFA decile ranking - Index of Economic Resources (IER) no.  \\\n",
       "CODE      LABEL     YEAR                                                                                                                    \n",
       "101021007 Braidwood 2014                                           5.455036                                                                 \n",
       "                    2015                                           5.455036                                                                 \n",
       "                    2016                                           6.000000                                                                 \n",
       "101021008 Karabar   2014                                           5.455036                                                                 \n",
       "                    2015                                           5.455036                                                                 \n",
       "\n",
       "                          SOCIO-ECONOMIC INDEXES FOR AREAS (SEIFA) State SEIFA decile ranking - Index of Relative Socio-Economic Disadvantage (IRSD) no.  \\\n",
       "CODE      LABEL     YEAR                                                                                                                                   \n",
       "101021007 Braidwood 2014                                           5.483813                                                                                \n",
       "                    2015                                           5.483813                                                                                \n",
       "                    2016                                           6.000000                                                                                \n",
       "101021008 Karabar   2014                                           5.483813                                                                                \n",
       "                    2015                                           5.483813                                                                                \n",
       "\n",
       "                          SOCIO-ECONOMIC INDEXES FOR AREAS (SEIFA) State SEIFA decile ranking - Index of Relative Socio-Economic Advantage and Disadvantage (IRSAD) no.  \\\n",
       "CODE      LABEL     YEAR                                                                                                                                                  \n",
       "101021007 Braidwood 2014                                           5.485612                                                                                               \n",
       "                    2015                                           5.485612                                                                                               \n",
       "                    2016                                           6.000000                                                                                               \n",
       "101021008 Karabar   2014                                           5.485612                                                                                               \n",
       "                    2015                                           5.485612                                                                                               \n",
       "\n",
       "                          SOCIO-ECONOMIC INDEXES FOR AREAS (SEIFA) State SEIFA decile ranking - Index of Education and Occupation (IEO) no.  \\\n",
       "CODE      LABEL     YEAR                                                                                                                      \n",
       "101021007 Braidwood 2014                                           5.497307                                                                   \n",
       "                    2015                                           5.497307                                                                   \n",
       "                    2016                                           7.000000                                                                   \n",
       "101021008 Karabar   2014                                           5.497307                                                                   \n",
       "                    2015                                           5.497307                                                                   \n",
       "\n",
       "                          SOCIO-ECONOMIC INDEXES FOR AREAS (SEIFA) State SEIFA decile ranking - Index of Economic Resources (IER) no.  \\\n",
       "CODE      LABEL     YEAR                                                                                                                \n",
       "101021007 Braidwood 2014                                           5.483813                                                             \n",
       "                    2015                                           5.483813                                                             \n",
       "                    2016                                           7.000000                                                             \n",
       "101021008 Karabar   2014                                           5.483813                                                             \n",
       "                    2015                                           5.483813                                                             \n",
       "\n",
       "                          HOMELESSNESS Homelessness rate per 10,000 persons rate  \\\n",
       "CODE      LABEL     YEAR                                                           \n",
       "101021007 Braidwood 2014                                          44.959294        \n",
       "                    2015                                          44.959294        \n",
       "                    2016                                          52.900000        \n",
       "101021008 Karabar   2014                                          44.959294        \n",
       "                    2015                                          44.959294        \n",
       "\n",
       "                          COMMUTING TO WORK Average commuting distance from Place of Usual Residence kms  \\\n",
       "CODE      LABEL     YEAR                                                                                   \n",
       "101021007 Braidwood 2014                                           18.20107                                \n",
       "                    2015                                           18.20107                                \n",
       "                    2016                                           38.00000                                \n",
       "101021008 Karabar   2014                                           18.20107                                \n",
       "                    2015                                           18.20107                                \n",
       "\n",
       "                          COMMUTING TO WORK Median commuting distance from Place of Usual Residence kms  \\\n",
       "CODE      LABEL     YEAR                                                                                  \n",
       "101021007 Braidwood 2014                                          12.379964                               \n",
       "                    2015                                          12.379964                               \n",
       "                    2016                                          19.500000                               \n",
       "101021008 Karabar   2014                                          12.379964                               \n",
       "                    2015                                          12.379964                               \n",
       "\n",
       "                          COMMUTING TO WORK Average commuting distance to place of work kms  \\\n",
       "CODE      LABEL     YEAR                                                                      \n",
       "101021007 Braidwood 2014                                          15.532562                   \n",
       "                    2015                                          15.532562                   \n",
       "                    2016                                          17.100000                   \n",
       "101021008 Karabar   2014                                          15.532562                   \n",
       "                    2015                                          15.532562                   \n",
       "\n",
       "                          COMMUTING TO WORK Median commuting distance to place of work kms  \n",
       "CODE      LABEL     YEAR                                                                    \n",
       "101021007 Braidwood 2014                                           8.646964                 \n",
       "                    2015                                           8.646964                 \n",
       "                    2016                                           4.200000                 \n",
       "101021008 Karabar   2014                                           8.646964                 \n",
       "                    2015                                           8.646964                 \n",
       "\n",
       "[5 rows x 218 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the files for exploration\n",
    "# Note: could optimize to import all the CSVs from that directory, not sure how to assign distinct dataframes \n",
    "# for each off the top of my head.\n",
    "df_income = pd.read_csv('{}\\CSV\\Income_ASGS_Final.csv'.format(nb_path), na_values='-', thousands=',')\n",
    "df_pop = pd.read_csv('{}\\CSV\\Population and People_ASGS.csv'.format(nb_path), na_values='-', thousands=',')\n",
    "df_solar = pd.read_csv('{}\\CSV\\Land and Environment_ASGS.csv'.format(nb_path), na_values='-', thousands=',')\n",
    "df_fam = pd.read_csv('{}\\CSV\\Family and Community_ASGS.csv'.format(nb_path), na_values='-', thousands=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fam['HOUSEHOLDS BY TYPE Total households no.'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = ['CODE',\n",
    "             'LABEL',\n",
    "             'YEAR',\n",
    "             'SOLAR INSTALLATIONS Small-Scale Solar Panel System Installations no.']\n",
    "\n",
    "drop_cols = []\n",
    "\n",
    "for column in df_solar.columns:\n",
    "    if not(column in keep_cols):\n",
    "        drop_cols.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_solar.drop(drop_cols,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purposes of this analysis, I'm only going to be looking at the most granular data.\n",
    "# This is in order to get the greatest number of unique data points for any ML to be applied.\n",
    "# Therefore we can drop any records with \"CODE\" length < 9. \n",
    "def removeaggregation(df):\n",
    "    df.drop(df[df['CODE'].astype(str).map(len) <= 8].index, inplace=True)\n",
    "    return df\n",
    "\n",
    "df_list = [df_income, df_solar, df_fam, df_pop]\n",
    "df_income, df_solar, df_fam, df_pop = [df.pipe(removeaggregation) for df in df_list]\n",
    "\n",
    "\n",
    "#df_income.drop(df_income[df_income['CODE'].astype(str).map(len) <= 8].index, inplace=True)\n",
    "#df_solar.drop(df_income[df_income['CODE'].astype(str).map(len) <= 8].index, inplace=True)\n",
    "#df_fam.drop(df_income[df_income['CODE'].astype(str).map(len) <= 8].index, inplace=True)\n",
    "#df_pop.drop(df_income[df_income['CODE'].astype(str).map(len) <= 8].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes together for analysis\n",
    "df = pd.merge(df_income, df_solar, how='inner', left_on=['CODE','YEAR','LABEL'], right_on=['CODE','YEAR','LABEL'])\n",
    "\n",
    "cols_to_use = df_pop.columns.difference(df.columns).tolist()\n",
    "cols_to_use.extend(['CODE','YEAR','LABEL'])\n",
    "\n",
    "df = pd.merge(df, df_pop[cols_to_use], how='inner', left_on=['CODE','YEAR','LABEL'], right_on=['CODE','YEAR','LABEL'])\n",
    "\n",
    "cols_to_use = df_fam.columns.difference(df.columns).tolist()\n",
    "cols_to_use.extend(['CODE','YEAR','LABEL'])\n",
    "\n",
    "df = pd.merge(df, df_fam, how='inner', left_on=['CODE','YEAR','LABEL'], right_on=['CODE','YEAR','LABEL'])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in multi-index?\n",
    "df.set_index(['CODE', 'LABEL', 'YEAR'], inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_series_abs(S):\n",
    "    'Takes a pandas Series object and returns the series sorted by absolute value'\n",
    "    temp_df = pd.DataFrame(S)\n",
    "    temp_df['abs'] = temp_df.iloc[:,0].abs()\n",
    "    temp_df.sort_values('abs', ascending = False, inplace = True)\n",
    "    return temp_df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_comparisons(df, num_cols):\n",
    "    '''\n",
    "    Takes a dataframe object and plots each row as a bar chart using the index as the \n",
    "    chart title and the other columns as X-axis labels, wihtin a 5x(n/5) subplot frame\n",
    "    df: dataframe object with values from which you want a chart of each row\n",
    "    num_cols: the number of charts per output row\n",
    "    '''\n",
    "    rows = df.shape[0]\n",
    "    columns = df.shape[1]\n",
    "    fig, axs = plt.subplots(nrows = int(rows/num_cols + rows%num_cols), ncols = num_cols, sharey = True,\n",
    "                           figsize=(15,int(rows*0.66)))\n",
    "    i=1\n",
    "    for row in axs:\n",
    "        for ax in row:\n",
    "            if i <= rows:\n",
    "                sns.barplot(data = df.iloc[i-1:i,:], ax = ax)\n",
    "                ax.title.set_text(df.index[i-1])\n",
    "            else:\n",
    "                ax.axis('off')\n",
    "            i += 1\n",
    "    plt.tight_layout()    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fun exploration, let's just run a histogram over everything!\n",
    "def histo_plots(df, num_cols):\n",
    "    '''\n",
    "    Takes a dataframe object and plots each column as a histogram chart using the feature title as the \n",
    "    chart title and the other columns as X-axis labels, within an n*(len/n) subplot frame\n",
    "    df: dataframe object with values from which you want a chart of each row\n",
    "    num_cols: the number of charts per output row\n",
    "    '''\n",
    "    rows = df.shape[0]\n",
    "    columns = df.shape[1]\n",
    "    plot_rows = (int(columns/num_cols) + ((columns%num_cols)!=0))\n",
    "    fig, axs = plt.subplots(nrows = plot_rows, \n",
    "                            ncols = num_cols, sharey = False, figsize=(15,columns))\n",
    "    nlines = 1\n",
    "\n",
    "    for i in range(axs.shape[0]*axs.shape[1]):\n",
    "        #print(i)\n",
    "        if i < columns:\n",
    "            try:\n",
    "                df.iloc[:,i].dropna().plot.hist(ax = axs[int(i/num_cols),int(i%num_cols)])\n",
    "                axs[int(i/num_cols),i%num_cols].set_title(\"\\n\".join(wrap(df.columns[i], int(100/num_cols))))\n",
    "                nlines = max(nlines,axs[int(i/num_cols),int(i%num_cols)].get_title().count('\\n'))\n",
    "            \n",
    "            except:\n",
    "                axs[int(i/num_cols),i%num_cols].set_title(\"\\n\".join(wrap(df.columns[i], 100/num_cols)))\n",
    "                axs[int(i/num_cols),i%num_cols].axis('off')\n",
    "        else:\n",
    "            axs[int(i/num_cols),int(i%num_cols)].axis('off')\n",
    "    \n",
    "    fig.subplots_adjust(hspace=0.5*nlines)\n",
    "    plt.tight_layout()    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#histo_plots(df.iloc[:,4:],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr().iloc[:,-1].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar = 'SOLAR INSTALLATIONS Small-Scale Solar Panel System Installations no.'\n",
    "sort_series_abs(df.dropna(subset=[solar]\n",
    "                         ).corr().loc[:,solar])[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Thoughts\n",
    "Very interesting that there is a high correlation between number of individuals on government pensions and those earning annuities, and even moreso that there is a *negative* correlation with the share of high income earners and number of solar panels, which also flows through into total household income.\n",
    "\n",
    "Obviously the first step I'm missing is *per household* solar panel information, as simply increasing the number of households in a given region also increases the chance there will be another solar panel installation. \n",
    "\n",
    "I'm also anticipating there will be some outlier suburbs etc. where there is zero probability of household solar panel installation, simply because there are no households (i.e. those comprised entirely of apartment blocks, if such a region exists).\n",
    "\n",
    "I also wonder if there is a reason to explore *change* in installations year on year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar = 'SOLAR INSTALLATIONS Small-Scale Solar Panel System Installations no.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_fam = pd.read_csv('{}\\CSV\\Family and Community_ASGS.csv'.format(nb_path), na_values='-', thousands=',')\n",
    "#df_fam = removeaggregation(df_fam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.merge(df, df_fam, how='inner', left_on=['CODE','YEAR','LABEL'], right_on=['CODE','YEAR','LABEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['HOUSEHOLDS BY TYPE Total households no.'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Solar Per Household'] = df[solar]/df['HOUSEHOLDS BY TYPE Total households no.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_series_abs(df.dropna(subset=[solar]\n",
    "                         ).corr().loc[:,solar])[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_series_abs(df.dropna(subset=['Solar Per Household']\n",
    "                         ).corr().loc[:,'Solar Per Household'])[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next level thoughts\n",
    "Okay, so the data at least makes sense - there is a strong negative correlation between population density and solar installations per household, owing to some key factors regarding building composition within the region:\n",
    "* More renters in the region result in less solar investment (they are unlikely to reap the long term capital benefits)\n",
    "* Greater number of apartment buildings\n",
    "* I'm not sure about this one, but number of households with neither rooms to spare nor rooms needed, so perfectly utilised. My best guess is this figure itself is closely associated with renting.\n",
    "\n",
    "Some other observations leap out at me:\n",
    "* The positive correlation with 55-64 year old residents\n",
    "* The negative correlation with immigrants and non-citizens\n",
    "* The negative correlation with overall income, but positive correlation with income between $500-999 per week\n",
    "* Positive correlations with income earners' ages\n",
    "* A *very* surprising negative correlation with gifts and donations! Goes to show that this particular \"green\" signal is not an indicator of altruistic behaviour.\n",
    "\n",
    "It is also interesting to note the overwhelming amount of strong negative correlations, solar has a lot going against it apparently!\n",
    "\n",
    "Annoyingly, there seem to be some direct duplicate columns in the dataset (note the \"no..1 and no..2\" variables) so I will need some way of eliminating these before running PCA and other ML techniques over the data to ensure that coefficients aren't distorted by this duplication.\n",
    "\n",
    "# But that's just correlations\n",
    "Time to delve into some more advanced modelling to see what jumps out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do linear regression and sort by coefficient per example investigation\n",
    "# Probably do PCA first given the *large* amount of overlapping features\n",
    "df.describe()\n",
    "#probably do some exploration of na values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = clean_data(df, 'Solar Per Household')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SOLAR INSTALLATIONS Small-Scale Solar Panel System Installations no.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, y_column, fill_mean_subset = None):\n",
    "    '''\n",
    "    A function to clean a dataframe and return X & y values for further processing. \n",
    "    Rows are removed where NaNs are present in response vector records.\n",
    "    NaN values for all other features are filled with the mean of the feature.\n",
    "    \n",
    "    INPUT\n",
    "    df - pandas dataframe \n",
    "    y_column - String. Name of column to be used as the response vector\n",
    "    fill_mean_subset - String, column name. Allows the input of a column to \"subset\" when first completing\n",
    "                        imputing missing numerical values with a series mean. E.g. if there is a categorical \n",
    "                        field of \"year\", allows imputing of null values with the mean of each year, rather \n",
    "                        than the mean of the overall series.  \n",
    "    \n",
    "    OUTPUT\n",
    "    X - A matrix holding all of the variables you want to consider when predicting the response\n",
    "    y - the corresponding response vector\n",
    "    '''    \n",
    "    # Remove duplicate columns\n",
    "    drop_cols = []\n",
    "    check_cols = df.columns.tolist()\n",
    "    check_cols.sort()\n",
    "    w_end = len(check_cols)\n",
    "    i = 0\n",
    "    \n",
    "    # Cycle through each column name\n",
    "    while i < w_end:\n",
    "        # assign a Check variable the the column name as a string\n",
    "        # that name string should only include characters up to 1 character after the final space\n",
    "        # e.g. \"* %\" or \"* n\"\n",
    "        check_str = check_cols[i]\n",
    "        check_str = check_str[:(check_str.rfind(\" \")+2)]\n",
    "        \n",
    "        for col in check_cols[(i+1):]:\n",
    "            # look forward in the list of column names for any other items matching CheckString & \"*\"\n",
    "            # add any matches to a list to drop, drop from the \"check\" list as well so make further searches more efficient.\n",
    "            # I'm almost certain there is a more efficient way to do this list/dict-wise\n",
    "            if col.startswith(check_str):\n",
    "                drop_cols.append(col)\n",
    "                check_cols.remove(col)\n",
    "                w_end -= 1\n",
    "        i += 1  \n",
    "    \n",
    "    df.drop(drop_cols, axis = 1, inplace=True)\n",
    "    \n",
    "    # Drop rows with missing response values\n",
    "    df = df.dropna(subset=[y_column], axis=0)\n",
    "    y = df[y_column]\n",
    "    df = df.dropna(how = 'all', axis = 1)\n",
    "    #Drop response column\n",
    "    df = df.drop(y_column, axis=1)\n",
    "    \n",
    "    # Fill numeric columns with the mean    \n",
    "    num_vars = df.select_dtypes(include=['float', 'int']).columns\n",
    "    # First, fill with the mean of the subset based on given category\n",
    "    if fill_mean_subset != None:\n",
    "        index_reset = False\n",
    "        index_names = list(df.index.names)\n",
    "        \n",
    "        # Filtering sucks with multi-indexing so temporarily reset the indexes for this action\n",
    "        if fill_mean_subset in index_names:\n",
    "            index_reset = True\n",
    "            df.reset_index(inplace=True)\n",
    "\n",
    "        #Check if subset variable is an index item\n",
    "        for subset_item in df[fill_mean_subset].unique().tolist():\n",
    "            for col in num_vars:\n",
    "                subset_mean = df[df[fill_mean_subset] == subset_item][col].mean() \n",
    "                df.loc[(df[fill_mean_subset] == subset_item) & (df[col].isnull()), col] = subset_mean\n",
    "        \n",
    "        if index_reset:\n",
    "            df.set_index(index_names, inplace=True)\n",
    "\n",
    "    # For any remaining nulls, fill with the mean of the overall series\n",
    "    for col in num_vars:\n",
    "        df[col].fillna((df[col].mean()), inplace=True)\n",
    "        \n",
    "    # OHE the categorical variables\n",
    "    cat_vars = df.select_dtypes(include=['object']).copy().columns\n",
    "    for var in  cat_vars:\n",
    "        # for each cat add dummy var, drop original column\n",
    "        df = pd.concat([df.drop(var, axis=1), pd.get_dummies(df[var], prefix=var, prefix_sep='_', drop_first=True)], axis=1)\n",
    "    \n",
    "    # Fill OHE NaNs with 0\n",
    "    # Get list of columns after OHE that were not in the \"numeric\" list from earlier, using set function for speed.\n",
    "    cat_vars = list(set(df.columns.tolist()) - set(num_vars.tolist()))\n",
    "    for var in cat_vars:\n",
    "        df[var].fillna(0, inplace=True)\n",
    "    \n",
    "    X = df\n",
    "    return X, y\n",
    "    \n",
    "#Use the function to create X and y\n",
    "X, y = clean_data(df.drop('SOLAR INSTALLATIONS Small-Scale Solar Panel System Installations no.', axis = 1), 'Solar Per Household', 'YEAR')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(y_pred, y_test)\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('Actuals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = eli5.sklearn.PermutationImportance(rf).fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.columns[114])\n",
    "print(X.columns[134])\n",
    "print(X.columns[195])\n",
    "print(X.columns[177])\n",
    "print(X.columns[110])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coef_weights(coefficients, X_train):\n",
    "    '''\n",
    "    INPUT:\n",
    "    coefficients - the coefficients of the linear model \n",
    "    X_train - the training data, so the column names can be used\n",
    "    OUTPUT:\n",
    "    coefs_df - a dataframe holding the coefficient, estimate, and abs(estimate)\n",
    "    \n",
    "    Provides a dataframe that can be used to understand the most influential coefficients\n",
    "    in a linear model by providing the coefficient estimates along with the name of the \n",
    "    variable attached to the coefficient.\n",
    "    '''\n",
    "    coefs_df = pd.DataFrame()\n",
    "    coefs_df['est_int'] = X_train.columns\n",
    "    coefs_df['coefs'] = lm_model.coef_\n",
    "    coefs_df['abs_coefs'] = np.abs(lm_model.coef_)\n",
    "    coefs_df = coefs_df.sort_values('abs_coefs', ascending=False)\n",
    "    return coefs_df\n",
    "\n",
    "#Use the function\n",
    "coef_df = coef_weights(lm_model.coef_, X_train)\n",
    "\n",
    "#A quick look at the top results\n",
    "coef_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_fit_linear_mod(df, response_col, test_size=.3, rand_state=42):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - a dataframe holding all the variables of interest\n",
    "    response_col - a string holding the name of the column\n",
    "    test_size - a float between [0,1] about what proportion of data should be in the test dataset\n",
    "    rand_state - an int that is provided as the random state for splitting the data into training and test\n",
    "\n",
    "    OUTPUT:\n",
    "    X - cleaned X matrix (dummy and mean imputation)\n",
    "    y - cleaned response (just dropped na)\n",
    "    test_score - float - r2 score on the test data\n",
    "    train_score - float - r2 score on the test data\n",
    "    lm_model - model object from sklearn\n",
    "    X_train, X_test, y_train, y_test - output from sklearn train test split used for optimal model\n",
    "\n",
    "    This function cleans the data and provides the necessary output for the rest of this notebook.\n",
    "    '''\n",
    "    #Dropping where the salary has missing values\n",
    "    df  = df.dropna(subset=['Salary'], axis=0)\n",
    "\n",
    "    #Drop columns with all NaN values\n",
    "    df = df.dropna(how='all', axis=1)\n",
    "\n",
    "    #Pull a list of the column names of the categorical variables\n",
    "    cat_df = df.select_dtypes(include=['object'])\n",
    "    cat_cols = cat_df.columns\n",
    "\n",
    "    #dummy all the cat_cols\n",
    "    for col in  cat_cols:\n",
    "        df = pd.concat([df.drop(col, axis=1), pd.get_dummies(df[col], prefix=col, prefix_sep='_', drop_first=True, dummy_na=True)], axis=1)\n",
    "\n",
    "\n",
    "    # Mean function\n",
    "    fill_mean = lambda col: col.fillna(col.mean())\n",
    "    # Fill the mean\n",
    "    df = df.apply(fill_mean, axis=0)\n",
    "\n",
    "    #Split into explanatory and response variables\n",
    "    X = df.drop(response_col, axis=1)\n",
    "    y = df[response_col]\n",
    "\n",
    "    #Split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=rand_state)\n",
    "\n",
    "    lm_model = LinearRegression(normalize=True) # Instantiate\n",
    "    lm_model.fit(X_train, y_train) #Fit\n",
    "\n",
    "    #Predict using your model\n",
    "    y_test_preds = lm_model.predict(X_test)\n",
    "    y_train_preds = lm_model.predict(X_train)\n",
    "\n",
    "    #Score using your model\n",
    "    test_score = r2_score(y_test, y_test_preds)\n",
    "    train_score = r2_score(y_train, y_train_preds)\n",
    "\n",
    "    return X, y, test_score, train_score, lm_model, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_lm_mod(X, y, cutoffs, test_size = .30, random_state=42, plot=True):\n",
    "    '''\n",
    "    INPUT\n",
    "    X - pandas dataframe, X matrix\n",
    "    y - pandas dataframe, response variable\n",
    "    cutoffs - list of ints, cutoff for number of non-zero values in dummy categorical vars\n",
    "    test_size - float between 0 and 1, default 0.3, determines the proportion of data as test data\n",
    "    random_state - int, default 42, controls random state for train_test_split\n",
    "    plot - boolean, default 0.3, True to plot result\n",
    "\n",
    "    OUTPUT\n",
    "    r2_scores_test - list of floats of r2 scores on the test data\n",
    "    r2_scores_train - list of floats of r2 scores on the train data\n",
    "    lm_model - model object from sklearn\n",
    "    X_train, X_test, y_train, y_test - output from sklearn train test split used for optimal model\n",
    "    '''\n",
    "    r2_scores_test, r2_scores_train, num_feats, results = [], [], [], dict()\n",
    "    for cutoff in cutoffs:\n",
    "\n",
    "        #reduce X matrix\n",
    "        reduce_X = X.iloc[:, np.where((X.sum() > cutoff) == True)[0]]\n",
    "        num_feats.append(reduce_X.shape[1])\n",
    "\n",
    "        #split the data into train and test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(reduce_X, y, test_size = test_size, random_state=random_state)\n",
    "\n",
    "        #fit the model and obtain pred response\n",
    "        lm_model = LinearRegression(normalize=True)\n",
    "        lm_model.fit(X_train, y_train)\n",
    "        y_test_preds = lm_model.predict(X_test)\n",
    "        y_train_preds = lm_model.predict(X_train)\n",
    "\n",
    "        #append the r2 value from the test set\n",
    "        r2_scores_test.append(r2_score(y_test, y_test_preds))\n",
    "        r2_scores_train.append(r2_score(y_train, y_train_preds))\n",
    "        results[str(cutoff)] = r2_score(y_test, y_test_preds)\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(num_feats, r2_scores_test, label=\"Test\", alpha=.5)\n",
    "        plt.plot(num_feats, r2_scores_train, label=\"Train\", alpha=.5)\n",
    "        plt.xlabel('Number of Features')\n",
    "        plt.ylabel('Rsquared')\n",
    "        plt.title('Rsquared by Number of Features')\n",
    "        plt.legend(loc=1)\n",
    "        plt.show()\n",
    "\n",
    "    best_cutoff = max(results, key=results.get)\n",
    "\n",
    "    #reduce X matrix\n",
    "    reduce_X = X.iloc[:, np.where((X.sum() > int(best_cutoff)) == True)[0]]\n",
    "    num_feats.append(reduce_X.shape[1])\n",
    "\n",
    "    #split the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reduce_X, y, test_size = test_size, random_state=random_state)\n",
    "\n",
    "    #fit the model\n",
    "    lm_model = LinearRegression(normalize=True)\n",
    "    lm_model.fit(X_train, y_train)\n",
    "\n",
    "    return r2_scores_test, r2_scores_train, lm_model, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cutoffs here pertains to the number of missing values allowed in the used columns.\n",
    "#Therefore, lower values for the cutoff provides more predictors in the model.\n",
    "cutoffs = [5000, 3500, 2500, 1000, 100, 50, 30, 25]\n",
    "\n",
    "#Run this cell to pass your X and y to the model for testing\n",
    "r2_scores_test, r2_scores_train, lm_model, X_train, X_test, y_train, y_test = find_optimal_lm_mod(X, y, cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do something else from supervised learning \n",
    "# AdaBoost baby! Given the large number of strong correlations above, and relatively unskewed historgram plots,\n",
    "# the main weakness of AdaBoost being sensitivity to outliers and noisy data is minimal.\n",
    "\n",
    "# Maybe try SVM while I'm at it, since the number of rows isn't huge, but the number of dimensions is\n",
    "\n",
    "# Use feature importance just like in the first project to get at this item"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
